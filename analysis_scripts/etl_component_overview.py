#!/usr/bin/env python3
"""
ETL Component Overview - Detailed Analysis of Each Script

This script provides a comprehensive overview of what each component does
in the current ETL process, in execution order.
"""

import ast
import re
from pathlib import Path
from datetime import datetime

def analyze_file_structure(file_path):
    """Analyze a Python file's structure and extract key information."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Parse the AST
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return {
                'classes': [],
                'functions': [],
                'imports': [],
                'docstring': None,
                'lines': len(content.split('\n'))
            }
        
        classes = []
        functions = []
        imports = []
        docstring = ast.get_docstring(tree)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                classes.append({
                    'name': node.name,
                    'methods': class_methods,
                    'docstring': ast.get_docstring(node)
                })
            elif isinstance(node, ast.FunctionDef) and not any(node in cls.body for cls in ast.walk(tree) if isinstance(cls, ast.ClassDef)):
                functions.append({
                    'name': node.name,
                    'docstring': ast.get_docstring(node)
                })
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                else:
                    module = node.module or ''
                    for alias in node.names:
                        imports.append(f"{module}.{alias.name}" if module else alias.name)
        
        return {
            'classes': classes,
            'functions': functions,
            'imports': imports[:10],  # Limit to first 10 imports
            'docstring': docstring,
            'lines': len(content.split('\n'))
        }
    except Exception as e:
        return {
            'error': str(e),
            'lines': 0
        }

def overview_etl_components():
    """Provide detailed overview of each ETL component in execution order."""
    
    print("üîç ETL COMPONENT OVERVIEW - EXECUTION ORDER")
    print("=" * 70)
    print(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Define components in execution order
    components = [
        {
            "order": 1,
            "name": "Main Entry Point",
            "file": "run_etl_pipeline.py",
            "purpose": "Command-line interface and pipeline orchestration",
            "role": "Entry point that parses arguments and starts the ETL process"
        },
        {
            "order": 2,
            "name": "ETL Orchestrator",
            "file": "etl_pipeline/core/orchestrator.py",
            "purpose": "Main coordinator of the entire ETL pipeline",
            "role": "Manages all phases: discovery, extraction, transformation, loading"
        },
        {
            "order": 3,
            "name": "Data Validator (Pre-processing)",
            "file": "etl_pipeline/validators/data_validator.py",
            "purpose": "Validates raw Excel files before processing",
            "role": "Ensures Excel files are readable and contain expected sheets"
        },
        {
            "order": 4,
            "name": "Data Extractor",
            "file": "etl_pipeline/core/extractor.py",
            "purpose": "Extracts data from raw Excel files",
            "role": "Handles sheet selection, header detection, and data extraction"
        },
        {
            "order": 5,
            "name": "Data Transformer",
            "file": "etl_pipeline/core/transformer.py",
            "purpose": "Cleans and transforms extracted data",
            "role": "Bank name canonicalization, data cleaning, panel creation"
        },
        {
            "order": 6,
            "name": "Bank Name Canonicalizer",
            "file": "etl_pipeline/utils/bank_name_canonicalizer.py",
            "purpose": "Standardizes bank names (partial)",
            "role": "Maps bank name variants to canonical forms"
        },
        {
            "order": 7,
            "name": "Data Validator (Post-processing)",
            "file": "etl_pipeline/validators/data_validator.py",
            "purpose": "Validates transformed data",
            "role": "Quality checks on panel dataset before loading"
        },
        {
            "order": 8,
            "name": "Data Loader",
            "file": "etl_pipeline/core/loader.py",
            "purpose": "Saves processed data and generates metadata",
            "role": "Writes CSV files, metadata, and summary reports"
        },
        {
            "order": 9,
            "name": "POST-PROCESSING: Bank Name Consolidation",
            "file": "analysis_scripts/consolidate_remaining_bank_names.py",
            "purpose": "Final bank name consolidation (manual step)",
            "role": "Handles remaining JSB variants and duplicate resolution"
        }
    ]
    
    # Analyze each component
    for component in components:
        print(f"\n{'='*70}")
        print(f"üîß {component['order']}. {component['name']}")
        print(f"{'='*70}")
        print(f"üìÅ File: {component['file']}")
        print(f"üéØ Purpose: {component['purpose']}")
        print(f"‚öôÔ∏è  Role: {component['role']}")
        
        # Analyze file structure
        file_path = Path(component['file'])
        if file_path.exists():
            analysis = analyze_file_structure(file_path)
            
            if 'error' not in analysis:
                print(f"üìä Lines of Code: {analysis['lines']}")
                
                if analysis['docstring']:
                    print(f"üìù Description: {analysis['docstring'][:200]}...")
                
                if analysis['classes']:
                    print(f"üèóÔ∏è  Classes ({len(analysis['classes'])}):")
                    for cls in analysis['classes']:
                        methods_count = len(cls['methods'])
                        print(f"   ‚Ä¢ {cls['name']} ({methods_count} methods)")
                        if cls['methods']:
                            key_methods = cls['methods'][:5]  # Show first 5 methods
                            print(f"     Key methods: {', '.join(key_methods)}")
                
                if analysis['functions']:
                    print(f"üîß Functions ({len(analysis['functions'])}):")
                    for func in analysis['functions'][:5]:  # Show first 5 functions
                        print(f"   ‚Ä¢ {func['name']}")
                
                if analysis['imports']:
                    key_imports = [imp for imp in analysis['imports'] if not imp.startswith('_')][:5]
                    if key_imports:
                        print(f"üì¶ Key Imports: {', '.join(key_imports)}")
            else:
                print(f"‚ùå Error analyzing file: {analysis['error']}")
        else:
            print(f"‚ùå File not found: {component['file']}")
        
        # Add specific insights for each component
        print(f"\nüí° Key Functionality:")
        if component['order'] == 1:
            print("   ‚Ä¢ Parses command-line arguments (data directory, output directory)")
            print("   ‚Ä¢ Sets up logging and configuration")
            print("   ‚Ä¢ Initializes and runs ETLOrchestrator")
            print("   ‚Ä¢ Handles errors and exit codes")
        
        elif component['order'] == 2:
            print("   ‚Ä¢ Discovers raw Excel files in input directory")
            print("   ‚Ä¢ Orchestrates all pipeline phases in sequence")
            print("   ‚Ä¢ Manages error handling and rollback")
            print("   ‚Ä¢ Generates comprehensive pipeline reports")
        
        elif component['order'] == 3:
            print("   ‚Ä¢ Validates Excel file readability")
            print("   ‚Ä¢ Checks for required sheets (assets, liabilities, etc.)")
            print("   ‚Ä¢ Estimates bank count per file")
            print("   ‚Ä¢ Pre-flight validation before extraction")
        
        elif component['order'] == 4:
            print("   ‚Ä¢ Intelligent sheet selection (fuzzy matching)")
            print("   ‚Ä¢ Dynamic header detection (row 4 vs 5)")
            print("   ‚Ä¢ Handles _NC (National Currency) sheet filtering")
            print("   ‚Ä¢ Date extraction from filenames")
        
        elif component['order'] == 5:
            print("   ‚Ä¢ Data type categorization (assets, liabilities, equity, results)")
            print("   ‚Ä¢ Column name cleaning and prefixing")
            print("   ‚Ä¢ Panel dataset creation (wide format)")
            print("   ‚Ä¢ Bank name canonicalization (partial)")
        
        elif component['order'] == 6:
            print("   ‚Ä¢ 200+ bank name mappings")
            print("   ‚Ä¢ Handles JSC/PJSC/CB variants (partial)")
            print("   ‚Ä¢ Removes aggregation rows")
            print("   ‚Ä¢ ‚ö†Ô∏è  Missing 4 critical JSB variants")
        
        elif component['order'] == 7:
            print("   ‚Ä¢ Panel dataset structure validation")
            print("   ‚Ä¢ Data quality checks (missing values, duplicates)")
            print("   ‚Ä¢ Column analysis and statistics")
            print("   ‚Ä¢ Bank-date combination validation")
        
        elif component['order'] == 8:
            print("   ‚Ä¢ Saves main panel dataset CSV")
            print("   ‚Ä¢ Generates metadata (JSON and text)")
            print("   ‚Ä¢ Creates summary statistics")
            print("   ‚Ä¢ Pipeline execution reports")
        
        elif component['order'] == 9:
            print("   ‚Ä¢ üö® MANUAL POST-PROCESSING STEP")
            print("   ‚Ä¢ Consolidates 4 remaining JSB bank pairs")
            print("   ‚Ä¢ Resolves duplicate bank-date combinations")
            print("   ‚Ä¢ Creates final CONSOLIDATED dataset")
            print("   ‚Ä¢ ‚ö†Ô∏è  This should be integrated into main pipeline!")

def analyze_data_flow():
    """Analyze the data flow through the pipeline."""
    
    print(f"\n{'='*70}")
    print("üìä DATA FLOW ANALYSIS")
    print(f"{'='*70}")
    
    flow_stages = [
        {
            "stage": "Input",
            "description": "Raw Excel files",
            "location": "data/raw_balance_sheets/*.xlsx",
            "format": "Excel workbooks with multiple sheets",
            "count": "~76 files (2019-2025)"
        },
        {
            "stage": "Discovery",
            "description": "File validation and discovery",
            "location": "ETL Orchestrator",
            "format": "File path list",
            "count": "~76 valid Excel files"
        },
        {
            "stage": "Extraction",
            "description": "Raw DataFrames per file",
            "location": "Data Extractor",
            "format": "pandas DataFrames",
            "count": "~304 DataFrames (4 types √ó 76 files)"
        },
        {
            "stage": "Transformation",
            "description": "Cleaned and categorized data",
            "location": "Data Transformer",
            "format": "Cleaned DataFrames with prefixed columns",
            "count": "~304 cleaned DataFrames"
        },
        {
            "stage": "Canonicalization",
            "description": "Partially standardized bank names",
            "location": "Bank Name Canonicalizer",
            "format": "DataFrames with canonical names",
            "count": "~200 bank name mappings applied"
        },
        {
            "stage": "Panel Creation",
            "description": "Wide panel dataset",
            "location": "Data Transformer",
            "format": "Single wide DataFrame",
            "count": "5,396 rows √ó 150+ columns"
        },
        {
            "stage": "Main Output",
            "description": "ETL pipeline output",
            "location": "output_final/ukrainian_banks_panel_dataset_FINAL.csv",
            "format": "CSV file",
            "count": "71 banks, 5,396 rows"
        },
        {
            "stage": "POST-PROCESSING",
            "description": "Manual consolidation step",
            "location": "analysis_scripts/consolidate_remaining_bank_names.py",
            "format": "Python script",
            "count": "4 bank pairs consolidated"
        },
        {
            "stage": "Final Output",
            "description": "Fully consolidated dataset",
            "location": "output_final/ukrainian_banks_panel_dataset_CONSOLIDATED.csv",
            "format": "CSV file",
            "count": "67 banks, 5,092 rows"
        }
    ]
    
    for i, stage in enumerate(flow_stages, 1):
        print(f"\n{i}. {stage['stage']}: {stage['description']}")
        print(f"   üìç Location: {stage['location']}")
        print(f"   üìÑ Format: {stage['format']}")
        print(f"   üìä Count: {stage['count']}")
        
        if stage['stage'] == "POST-PROCESSING":
            print("   üö® ISSUE: This should be integrated into main pipeline!")

def identify_integration_points():
    """Identify specific points where integration is needed."""
    
    print(f"\n{'='*70}")
    print("üéØ INTEGRATION POINTS")
    print(f"{'='*70}")
    
    integration_points = [
        {
            "component": "Bank Name Canonicalizer",
            "file": "etl_pipeline/utils/bank_name_canonicalizer.py",
            "current_state": "Handles ~200 mappings but missing 4 JSB variants",
            "needed_integration": "Add the 4 missing JSB mappings from post-processing script",
            "specific_mappings": [
                "'Clearing House JSB' ‚Üí 'Clearing House'",
                "'Industrialbank JSB' ‚Üí 'Industrialbank'", 
                "'Pivdennyi JSB' ‚Üí 'Pivdenny Bank'",
                "'Ukrg–∞zb–∞nk JSB' ‚Üí 'Ukrgasbank'"
            ]
        },
        {
            "component": "Data Transformer",
            "file": "etl_pipeline/core/transformer.py",
            "current_state": "Creates panel dataset but doesn't resolve duplicates",
            "needed_integration": "Add duplicate resolution logic from post-processing script",
            "specific_functionality": [
                "Detect duplicate bank-date combinations",
                "Resolve by keeping record with more complete data",
                "Update row counts and statistics"
            ]
        },
        {
            "component": "Data Validator",
            "file": "etl_pipeline/validators/data_validator.py",
            "current_state": "Validates structure but not consolidated names",
            "needed_integration": "Add validation for consolidated bank names",
            "specific_checks": [
                "Verify no JSB variants remain",
                "Check for duplicate bank-date combinations",
                "Validate final bank count (should be 67)"
            ]
        }
    ]
    
    for point in integration_points:
        print(f"\nüîß {point['component']}")
        print(f"   üìÅ File: {point['file']}")
        print(f"   üìä Current: {point['current_state']}")
        print(f"   üéØ Needed: {point['needed_integration']}")
        
        if 'specific_mappings' in point:
            print(f"   üìù Specific mappings to add:")
            for mapping in point['specific_mappings']:
                print(f"      ‚Ä¢ {mapping}")
        
        if 'specific_functionality' in point:
            print(f"   ‚öôÔ∏è  Specific functionality to add:")
            for func in point['specific_functionality']:
                print(f"      ‚Ä¢ {func}")
        
        if 'specific_checks' in point:
            print(f"   ‚úÖ Specific checks to add:")
            for check in point['specific_checks']:
                print(f"      ‚Ä¢ {check}")

if __name__ == "__main__":
    print("üöÄ Ukrainian Bank ETL - Component Overview")
    print("=" * 70)
    
    # Run comprehensive analysis
    overview_etl_components()
    analyze_data_flow()
    identify_integration_points()
    
    print(f"\n{'='*70}")
    print("‚úÖ SUMMARY")
    print(f"{'='*70}")
    print("Current ETL Pipeline: 8 components + 1 manual post-processing step")
    print("Goal: Integrate post-processing into main pipeline for single-command execution")
    print("Key Integration: Bank name mappings + duplicate resolution logic")
    print("Expected Result: Direct output of CONSOLIDATED dataset (67 banks, ~5,092 rows)") 